{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myndaaa/Natural-Language-Processing/blob/main/FNLP_Tutorial3_102773618.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8x8HCLqUAA0"
      },
      "source": [
        "\n",
        "#**COS30018 FNLP Tutorial 3**\n",
        "\n",
        "##**Lab Task Submission at bottom #PassTask2**\n",
        "## **Name: Mysha Nahiyan Shemontee**\n",
        "## **Student ID:102773618**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmJ08VS2OVwm"
      },
      "source": [
        "# Lab Submission 2 (Due Sunday Week 4, 11:59 PM)\n",
        "\n",
        "1. Attempt to use what you've learned to perform TF-IDF on the stories dataset. Select at least two documents (txt). You can use sklearn, nltk and any other libraries. Explain any findings can you obtain\n",
        "\n",
        "Make sure at the bare minimum to show the following:\n",
        "*   Remove stopwords\n",
        "*   Remove punctuations\n",
        "*   Show top rankings (IF POSSIBLE)\n",
        "*   Show TF-IDF values of words\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "Hfcl_4LH_D4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBOeBrAvlNmS"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# List of symbol for preprocessing\n",
        "list_of_symbols = [\"™\", \"®\", \"©\", \"&trade;\", \"&reg;\", \"&copy;\", \"&#8482;\", \"&#174;\", \"&#169;\", \"\\n\"]\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    Function to clean and preprocess text\n",
        "    :param text: Input text\n",
        "    :return: List of cleaned words\n",
        "    \"\"\"\n",
        "    # Remove punctuations\n",
        "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "\n",
        "    # remove other symbols\n",
        "    for symbol in list_of_symbols:\n",
        "        text = text.replace(symbol, \" \")\n",
        "\n",
        "    # remove unicode\n",
        "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
        "\n",
        "    # tokenize\n",
        "    tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
        "    words = tokenizer.tokenize(text)\n",
        "\n",
        "    # stemming and removing stopwords\n",
        "    stemmer = PorterStemmer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    stemmed_words = [stemmer.stem(word) for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    return stemmed_words\n",
        "\n",
        "# example if use of function\n",
        "text_to_process = \"How to use this function, just write text, punctuations and symbols like ™, and some stopwords.\"\n",
        "processed_text = text_preprocessing(text_to_process)\n",
        "print(processed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSagaieIlOWM"
      },
      "source": [
        "Hint to unzip:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing zipfile module\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# loading the temp.zip and creating a zip object\n",
        "with ZipFile(\"D:\\stories.zip\", 'r') as zObject:\n",
        "\tzObject.extractall(\n",
        "\t\tpath=\"D:\\\\unzippedFiles\")"
      ],
      "metadata": {
        "id": "TXrbWXy0D-7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "second txt file"
      ],
      "metadata": {
        "id": "Fc7EcRNjEAxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing zipfile module\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# loading the temp.zip and creating a zip object\n",
        "with ZipFile(\"D:\\wiki_movie_plots_deduped.csv.zip\", 'r') as zObject:\n",
        "\tzObject.extractall(\n",
        "\t\tpath=\"D:\\\\unzippedFiles\")"
      ],
      "metadata": {
        "id": "cJKuQ6oUECI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-o1zAZElT14"
      },
      "source": [
        "Hint to load text files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED3tcuJFALo3"
      },
      "outputs": [],
      "source": [
        "f = open(\"stories\\\\3gables.txt\", \"r\")\n",
        "\n",
        "#read whole file to a string\n",
        "data = f.read()\n",
        "\n",
        "#close file\n",
        "f.close()\n",
        "\n",
        "f = open(\"stories\\\\3lpigs.txt\", \"r\")\n",
        "\n",
        "#read whole file to a string\n",
        "data2 = f.read()\n",
        "\n",
        "#close file\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)\n",
        "print(data2)"
      ],
      "metadata": {
        "id": "fgLwH4nWFARt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy0s4TpolZi3"
      },
      "source": [
        "Hint to append data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX8oDfvMAq5I"
      },
      "outputs": [],
      "source": [
        "dataset = data + data2\n",
        "dataset_a = textpreprocessing(data)\n",
        "dataset_b = textpreprocessing(data2)\n",
        "dataset_list = textpreprocessing(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_list)"
      ],
      "metadata": {
        "id": "M0A0l72sE-cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU6WrDXjlmPU"
      },
      "source": [
        "Hint to show DF of TF-IDF of two documents:\n",
        "\n",
        "**CAUTION THIS MAY CAUSE MEMORY ERROR IF TOO MANY DOCUMENTS ARE USED**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgo_jisUNS6z"
      },
      "outputs": [],
      "source": [
        "df_tfidfvect = pd.DataFrame(data = TfidfVectorizer.toarray(),index = ['Doc1','Doc2'],columns = tfidf_tokens)\n",
        "df_tfidfvect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_frequency_matrix(words_list):\n",
        "    freq_table = {}\n",
        "    for word in words_list:\n",
        "\n",
        "        if word in freq_table:\n",
        "            freq_table[word] += 1\n",
        "        else:\n",
        "            freq_table[word] = 1\n",
        "    return freq_table\n",
        "\n",
        "freq_matrix = _create_frequency_matrix(dataset_list)\n",
        "freq_matrix"
      ],
      "metadata": {
        "id": "QTWOJcy0rfZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrxnzdGkNeX7"
      },
      "outputs": [],
      "source": [
        "def _create_tf_matrix(freq_matrix):\n",
        "    tf_matrix = {}\n",
        "    number_of_words = len(freq_matrix)\n",
        "    for item in freq_matrix:\n",
        "        tf_value = float(freq_matrix[item])/float(number_of_words)\n",
        "\n",
        "        tf_matrix[item] = tf_value\n",
        "    return tf_matrix\n",
        "\n",
        "tf_matrix  = _create_tf_matrix(freq_matrix)\n",
        "tf_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dictionary in desending order"
      ],
      "metadata": {
        "id": "cWcz3UGkGImw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_dict = dict(sorted(tf_matrix.items(), key=lambda x: x[1], reverse=True))\n",
        "sorted_dict #printing"
      ],
      "metadata": {
        "id": "CH_OXDxrF7jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def _create_idf_matrix(freq_matrix, document1, document2):\n",
        "    idf_matrix = {}\n",
        "    for item in freq_matrix:\n",
        "        if item in dataset_a and dataset_b:\n",
        "            document_instance = 2\n",
        "        else:\n",
        "            document_instance = 1\n",
        "        idf_value = math.log10(2.0/float(document_instance))\n",
        "        print(item,idf_value)\n",
        "        idf_matrix[item] = idf_value\n",
        "    return idf_matrix"
      ],
      "metadata": {
        "id": "Ma2MlvVnGQfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "if \"I\" in dataset_a and dataset_b:\n",
        "    document_instance = 2\n",
        "else:\n",
        "    document_instance = 1\n",
        "idf_value = math.log10(2.0/float(document_instance))\n",
        "idf_value"
      ],
      "metadata": {
        "id": "cnmzbv8mGV3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf_matrix = _create_idf_matrix(sorted_dict, dataset_a, dataset_b) #printing"
      ],
      "metadata": {
        "id": "NV6YenK8GgmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dictionary in descending order"
      ],
      "metadata": {
        "id": "AOQVyVCEGm_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_idf = dict(sorted(idf_matrix.items(), key=lambda x: x[1], reverse=True))\n",
        "sorted_idf"
      ],
      "metadata": {
        "id": "Dmx4EM2BGiFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
        "    tf_idf_matrix = {}\n",
        "    for item in tf_matrix:\n",
        "        tf_idf_value = float(tf_matrix[item]) * float(idf_matrix[item])\n",
        "        print(item,tf_idf_value)\n",
        "        tf_idf_matrix[item] = tf_idf_value\n",
        "    return tf_idf_matrix"
      ],
      "metadata": {
        "id": "74Shjpu_GrGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_dict = _create_tf_idf_matrix(tf_matrix, idf_matrix)"
      ],
      "metadata": {
        "id": "aik8owaKGvcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary in descending order\n",
        "sorted_tf_idf_dict = dict(sorted(tf_idf_dict.items(), key=lambda x: x[1], reverse=True))\n",
        "sorted_tf_idf_dict"
      ],
      "metadata": {
        "id": "da4JKiilGwQI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}