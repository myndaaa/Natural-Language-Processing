{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myndaaa/Natural-Language-Processing/blob/main/FNLP_Tutorial5e_PassTask4_102773618.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **COS30018 Tutorial 5 - Pass Task**\n",
        "\n",
        "##**Lab Task Submission #PassTask4**\n",
        "## **Name:Mysha Nahiyan Shemontee**\n",
        "## **Student ID:102773618**\n",
        "\n",
        "1. This week's pass task is slight different than before. We will not perform coding here but a written analysis. It will require you to look and cite for at least 2 papers to compare between at least two out of 3 of the word vector models (word2vec, GloVE, fasttest)\n",
        "\n",
        "Share your findings from the papers that includes the performance difference and timing difference if available. Indicative word count (50-250 words)"
      ],
      "metadata": {
        "id": "KAJNldSDg_rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Word2vec vs GloVE vs fast text**"
      ],
      "metadata": {
        "id": "RVc9HmsK2d3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OqFDt12K2oMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In recent times, natural language processing (NLP) has witnessed\n",
        "significant advancements, mostly in the domain of word embedding\n",
        "methods. These methods play an imp. role in various NLP tasks, including text classification, semantic similarity, and language translation. Among the prominent word embedding techniques are Word2Vec, GloVe, and FastText, each offers us unique approaches to represent words as dense vectors in a continuous vector space."
      ],
      "metadata": {
        "id": "G1JXbzNZ2yoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison in Text Classification**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZTYeBLjF3F5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A study done by Dharma et al. (2022) aimed to compare the performance of Word2Vec, GloVe, and FastText in the context of convolutional neural network (CNN) text classification. The evaluation was conducted on the UCI KDD archive dataset, consisting of 19,977 news stories distributed across 20 topics. The selection of these embedding methods was based on their ability to capture semantic, syntactic, sequential, and contextual information of words."
      ],
      "metadata": {
        "id": "Du--xk0b3I2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outcomes**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8ulQCifV3VtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of the testing showed that FastText outperformed Word2Vec and GloVe in terms of accuracy, achieving an accuracy rate of 97.2%. GloVe followed with an accuracy of 95.8%, while Word2Vec lagged behind with an accuracy of 92.5%. Although FastText demonstrated superior performance, the differences in accuracy between the embedding methods were not deemed significantly crucial. This variance in performance was attributed to the distinct architectures of each embedding method."
      ],
      "metadata": {
        "id": "MiTZE6lF3bW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challenges**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_5CC6LFQ32mO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One vital challenge encountered by Word2Vec and GloVe, as seen from the analysis was the representation of word vectors when encountering out-of-corpus words. this has posed a limitation in their performance."
      ],
      "metadata": {
        "id": "JtMuGIKy36Nx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deductions from report**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dxW_DrTS4Hso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can say that, [1] concluded that the choice of embedding method significantly influences the accuracy performance, depending on the characteristics of the provided dataset. Wheareas FastText came out as the top performer in this study, Word2Vec and GloVe also exhibited competitive accuracy rates. as A result,  researchers and practitioners must carefully consider the nature of the dataset and the specific task at hand when selecting an appropriate word embedding method."
      ],
      "metadata": {
        "id": "CXpB9a9l4Jfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experimental Testing with Turkish Datasets**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cW3oqxVx4vj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another experimental study was chosen as the 2nd paper for the passtask[2]. this one focused on evaluating the performance of Word2Vec, GloVe, and FastText on Turkish words and datasets. The comparative analysis utilized three benchmark datasets: SimTurk, AnlamVer, and RG64_Turkce."
      ],
      "metadata": {
        "id": "Nthg7ejZ4zxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8B-7tnWe41qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In contrast to the findings of [1],  [2] reported better correlation results between GloVe and FastText with Turkish word vectors in terms of semantic similarity. FastText also showed superiority in handling limited out-of-vocabulary (OOV) words, contributing to its enhanced performance. Furthermore, the SimTurk and AnlamVer datasets were seen to be better suited to FastText and GloVe vectors based on Spearman correlation values."
      ],
      "metadata": {
        "id": "kHXPoNSz45B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deductions**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wUHWXu2x46ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the paper mentioned that choice of embedding method also plays a role in the context of Turkish language processing. While FastText and GloVe exhibited better performance in certain aspects, the selection should be tailored to the specific characteristics of the Turkish language and the requirements of the task at hand."
      ],
      "metadata": {
        "id": "TkGZ8GvJ487T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overall Implications and conclusion**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Y7KVtRdf4_0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both these studies highlight the importance of considering various factors, including dataset characteristics, language nuances, and task requirements, when selecting an appropriate word embedding method. Further research in this area is necessary to explore the effective ness of different embedding techniques across diverse languages and different datasets."
      ],
      "metadata": {
        "id": "9qjjyOeH5CLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "1. Dharma, Gaol, Soewito, & Warnars. (2022). THE ACCURACY COMPARISON AMONG WORD2VEC, GLOVE, AND FASTTEXT TOWARDS CONVOLUTION NEURAL NETWORK (CNN) TEXT CLASSIFICATION. Journal of Theoretical and Applied Information Technology, 31 January 2022, 100(2):349-359, 100(18173195), 349–359.\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "2. Tulu, C. (2022, October 1). Experimental Comparison of Pre-Trained Word Embedding Vectors of Word2Vec, Glove, FastText for Word Level Semantic Text Similarity Measurement in Turkish. Advances in Science and Technology Research Journal, 16(4), 147–156. https://doi.org/10.12913/22998624/152453\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "47VqDoxl7H8v"
      }
    }
  ]
}